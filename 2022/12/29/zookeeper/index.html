<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
  <title>
    
    zookeeper 丨
    

    Lism Blog
  </title>

  
  <link rel="shortcut icon" href="/icon.svg">
  

  <link rel="preconnect" href="https://cdnjs.cloudflare.com">
  
  <link id="theme" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.css">
  <script src="https://unpkg.com/@highlightjs/cdn-assets@11.9.0/highlight.min.js"></script>
  

  <!-- 字体 -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">

  
<link rel="stylesheet" href="/css/root.css">

  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/css/post.css">

<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="/Lism Blog" type="application/atom+xml">
</head>

<body>
  <header class="header">
  <section class="header-container">
    <a class="logo" href="/">/Lism Blog</a>
    <ul class="nav">
      
      <li><a href="/archives">archives</a></li>
      
      <li><a href="/about">about</a></li>
      
    </ul>
  </section>
</header>
  <main class="main">
    <article class="post">
  
  <div class="post-title">zookeeper</div>
  <div class="post-meta">
    <div class="date">2022 十二月 29日</div>
    <div class="tags">
      
      <div class="tag-item">zookeeper</div>
      
    </div>
  </div>
  

  <main class="post-content"><h1 id="zookeeper"><a class="markdownIt-Anchor" href="#zookeeper"></a> zookeeper</h1>
<p>[toc]</p>
<h2 id="zookeeper概述"><a class="markdownIt-Anchor" href="#zookeeper概述"></a> Zookeeper概述</h2>
<h3 id="zookeeper简介"><a class="markdownIt-Anchor" href="#zookeeper简介"></a> Zookeeper简介</h3>
<p>zookeeper是一个开源的分布式应用程序协调服务器，其为分布式系统提供一致性服务。其一致性是通过基于Paxos算法的ZAB协议完成的，其主要功能包括：配置维护，域名服务，分布式同步，集群管理等。</p>
<h4 id="功能简介"><a class="markdownIt-Anchor" href="#功能简介"></a> 功能简介</h4>
<p><strong>配置维护</strong></p>
<p>分布式系统中，很多服务都是部署在集群中的，即多台服务器中部署着完全相同的应用，起着完全相同的作用，当然，集群中的这些服务器的配置文件是完全相同的。</p>
<p>若集群中服务器的配置文件需要进行更改，那么我们就需要逐台秀太这些服务器中的配置，数千台的服务器，手工维护的风险是很大的</p>
<p>这时候zookeeper就可以排上用场了，其对于配置文件的维护采用的是“发布/订阅模式”，发布者将修改好的集群的配置文件发布到zookeeper服务器的文件系统中，那么订阅者马上就可以接收到通知，并主动同步zookeeper里的配置文件，zookeeper具有同步操作的原子性，确保每个服务器的配置文件都可以被正确的修改</p>
<p><strong>域名服务</strong></p>
<p>在分布式应用中，一个项目包含多个工程，而这些工程中，有些工程是专门为其他工程提供服务的，一个项目中可能会存在多种提供不同服务的工程，而一种服务有可能存在多个提供者（服务器），所以，用于消费这些服务的客户端工程若要消费这些服务，就变得异常复杂</p>
<p>此时，zookeeper就可以上场了，为每个服务起个名称，将这些服务的名称与通过这些服务的主机地址注册都zookeeper中，形成一个服务映射表，服务消费者只需要通过服务名称即可享受到服务，而无需了解服务具体的提供者是谁，服务的减少，添加，变更，只需要改zookeeper中的服务映射表即可</p>
<p>阿里的Dobbo就是使用zookeeper作为服务域名服务器的</p>
<p><strong>分布式同步</strong></p>
<p>在分布式系统中，很多运算(对请求的处理)过程是由分布式机群中的若干服务器共同计算完成的，并且它们之间的运算还具有逻辑上的先后顺序。如何保证这些服务器运行期间的同步性呢? <br />
      使用Zookeeper可以协调这些服务器间运算的过程。让这些服务器都同时监听Zookeeper上的同一个znode (Zookeeper文件系统中的一个数据存储节点)，一旦其中一个服务器Update了znode, 那么另一个相应服务器能够收到通知，并作出相应处理。</p>
<p><strong>集群管理</strong></p>
<p>集群管理中最麻烦的就是节点故障管理。Zookeeper可以让集群选出一个健康的节点作为Master, Master随时监控着当前集群中的每个节点的健康状况，一旦某个节点发生故障，Master会把这个情况立即通知给集群中的其它节点，使其它节点对于任务的分配做出相应调整。Zookeeper不仅可以发现故障，也会对故障进行甄别，如果该故障可以修复，Zookeeper可以自动修复，若不能修复则会告诉系统管理员错误的原因让管理员迅速定位问题。。<br />
      但这里也有个问题: Master 故障了，那怎么办? Zookeeper内部有一个“选举算法”， 当Master故障出现时，Zookeeper 能马上选出新的Master对集群进行管理</p>
<h4 id="一致性要求"><a class="markdownIt-Anchor" href="#一致性要求"></a> 一致性要求</h4>
<p>什么是zk的一致性呢？就需要满足以下几点要求</p>
<p><strong>顺序一致性</strong></p>
<p>从同一个客户端发起的n多个事务请求（写请求），最终将会严格按照其发起顺序被应用到zookeeper中</p>
<p><strong>原子性</strong></p>
<p>所有事务请求的结果在集群中所有机器上的应用情况是一致的。也就是说要么整个集群所有主机都成功应用了某一个事务，要么都没有应用，不会出现集群中部分主机应用了该事务，而另外一部分没有应用的情况。</p>
<p><strong>单一视图</strong></p>
<p>无论客户端连接的是哪个ZooKeeper服务器，其看到的服务端数据模型都是一致的。</p>
<p><strong>可靠性</strong></p>
<p>一且服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更。</p>
<p><strong>实时性</strong></p>
<p>通常人们看到实时性的第一反应是，一旦一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。</p>
<h3 id="zookeeper中的重要概念"><a class="markdownIt-Anchor" href="#zookeeper中的重要概念"></a> zookeeper中的重要概念</h3>
<h4 id="session"><a class="markdownIt-Anchor" href="#session"></a> Session</h4>
<p>Session是指客户端会话。</p>
<p>ZooKeeper 对外的服务端口<strong>默认是2181</strong>，客户端启动时，首先会与zk服务器建立一个TCP长连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个长连接，客户端能够通过心跳检测保持与服务器的有效会话，也能够向ZooKeeper服务器发送请求并接受响应，同时还能通过该连接接收来自服务器的Watcher事件通知。</p>
<p>Session的<strong>SessionTimeout值用来设置一个客户端会话的超时时间</strong>。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在SessionTimeout规定的时间内客户端能够重新连接上集群中任意台服务器，那么之前创建的会话仍然有效。</p>
<h4 id="znode"><a class="markdownIt-Anchor" href="#znode"></a> znode</h4>
<p>zookeeper的文件系统采用树形层次化的目录结构，与Unix文件系统非常相似。每个目录在zookeeper中叫做一个znode,每个znode拥有一个唯一的路径标识，即名称。Znode可以包含数据和子znode (临时节点不能有子znode)。Znode 中的数据可以有多个版本，所以查询某路径下的数据需带上版本号。客户端应用可以在znode上设置监视器(Watcher)</p>
<h4 id="watcher机制"><a class="markdownIt-Anchor" href="#watcher机制"></a> Watcher机制</h4>
<p>zk通过<strong>Watcher机制实现了发布/订阅模式</strong>。k提供了分布式数据的发布订阅功能，一个发布者能够让多个订阅者同时监听某一主题对象， 当这个主题对象状态发生变化时，会通知所有订阅者，使它们能够做出相应的处理。zk引入了watcher机制来实现这种分布式的通知功能。zk允许客户端向服务端注册一个Watcher监听，当服务端的一些指定事件触发这个Watcher,那么就会向指定客户端发送一个事件通知。而这个事件通知则是通过TCP长连接的Session完成的</p>
<h4 id="acl"><a class="markdownIt-Anchor" href="#acl"></a> ACL</h4>
<p>ACL全称为Access Control List (访问控制列表),<strong>用于控制资源的访问权限，是zk数据安全的保障</strong>。zk利用ACL策略控制znode节点的访问权限，如节点数据读写、节点创建、节点删除、读取子节点列表、设置节点权限等。</p>
<p>在传统的文件系统中，ACL分为两个维度:组与权限。一个属组可以包含多种权限，一个文件或目录拥有了某个组的权限即拥有了组里的所有权限。文件或子目录默认会继承其父目录的ACL。</p>
<p>而在Zookeeper中，znode的ACL是没有继承关系的，每个znode的权限都是独立控制的，只有客户端满足znode设置的权限要求时，才能完成相应的操作。Zookeeper 的ACL分为三个维度：<strong>授权策略scheme，用户id，用户权限permission</strong></p>
<h3 id="paxos算法"><a class="markdownIt-Anchor" href="#paxos算法"></a> Paxos算法</h3>
<h4 id="算法简介"><a class="markdownIt-Anchor" href="#算法简介"></a> 算法简介</h4>
<p>Paxos算法是莱斯利&quot;伯特(Leslie tamport)1990年提出的一种基于消息传递的、具有高容错性的一致性算法。Google Chubby (分布式锁服务)的作者Mike Burrows说过，世上只有一种一致性算法，那就是Paxos, 所有其他致性算法 都是Paxos算法的不完整版。Paxos算法是一种公认的晦涩难懂的算法，并且工程实现上也具有很大难度。较有名的Paxos工程实现有Google Chubby、ZAB、 微信的PhxPaxos等。，</p>
<p>Paxos算法是用于解决什么问题的呢? Paxos 算法要解决的问题是，在分布式系统中如何就某个决议达成一致。</p>
<h4 id="paxos与拜占庭将军问题"><a class="markdownIt-Anchor" href="#paxos与拜占庭将军问题"></a> Paxos与拜占庭将军问题</h4>
<p>拜占庭将军问题(Byzantine failures),是由Paxos算法作者莱斯利兰伯特提出的点对点通信中的基本问题。该问题要说明的含义是，在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。</p>
<p>Paxos算法的前提是不存在拜占庭将军问题，即信道是安全的、可靠的，集群节点间传  国中J递的消息是不会被篡改的。在实际工程实践中，大多数系统都是部署在一个局域网内，因此消息被篡改的情况很少:另一方面，由于硬件和网络原因而造成的消息不完整问题，现在已经不再是问题，只需要一套简单的校验算法即可，因此，在实际工程中各个服务器间传递过程可以认为不存在拜占庭将军问题</p>
<p>一般情况下，分布式系统中各个节点间采用两种通讯模型，共享内存（shared Memory）,消息传递（Messages Passing）。而Paxos是基于消息传递通讯模型的</p>
<h4 id="算法描述"><a class="markdownIt-Anchor" href="#算法描述"></a> 算法描述</h4>
<h5 id="三种角色"><a class="markdownIt-Anchor" href="#三种角色"></a> 三种角色</h5>
<p>在Paxos算法中有三种角色，分别具有三种不同的行为。但很多时候，一一个进程可能同时充当着多种角色。</p>
<ul>
<li>Proposer:提案(Proposal) 的提议者。</li>
<li>Acceptor:提案的表决者，即是否accept该提案。只有半数以上的Acceptor接受了某提案，那么该提案者被认定为“选定”。</li>
<li>Learners:提案的学习者。当提案被选定，其会同步并执行提案</li>
</ul>
<p>一个提案的表决者(Acceptor)会存在多个，但在一个集群中，提议者(Proposer)是可能存在多个的，不同的提议者（Proposer）会提出不同的提案。而一致性算法则可以保证如下几点：</p>
<ul>
<li>没有提案被提出则不会有提案被选定</li>
<li>每个提议者在提出提案时都会为该提案指定一个具有全局唯性的、 递增的提案编号 N,即在整个集群中是唯一的。</li>
<li>每个表决者在accept某提案后，会将该提案的编号N记录在本地，这样每个表决者中保存的已经被accept的提案中会存在一个编号最大的提案，其编号假设为maxN。每个表决者仅会accept编号大于自己本地maxN的提案。</li>
<li>在众多提案中最终只能有一个提案被选定。</li>
<li>一旦一个提案被选定，则其它服务器会主动同步(Learn)该是案到本地。</li>
</ul>
<h5 id="算法过程描述"><a class="markdownIt-Anchor" href="#算法过程描述"></a> 算法过程描述</h5>
<p>Paxos算法的执行过程划分为两个阶段：准备阶段prepare与接受阶段accept</p>
<ul>
<li><strong>prepare 阶段</strong></li>
</ul>
<p>提议者(Proposer)准备提交一个编号为 N的提议， 于是其 首先向所有表决者(Acceptor)发送prepare(N)请求，用于试探集群是否支持该编号的提议。</p>
<p>每个表决者(Acceptor)中 都保存着自己曾经accept过的提议中的最大编号maxN。当一个表决者接收到其它主机发送来的prepare(N)请求时，其会比较N与maxN的值。若N小于等于maxN,则说明该提议己过时，当勤表决者采取不回应或回应Error的方式来拒绝该prepare请求;若N大于maxN,则说明该提议是可以接受的，当前表决者会将其曾经已经accept的编号最大的提案Proposal(mid,maxN,value)反馈给提议者，以向提议者展示自己支持的提案意愿。其中第一个参数mid表示表决者Acceptor的标识id,第二个参数表示其曾接受的提案的最大编号maxN,第三个参数表示该提案的真正内容value.当然，若当前表决者还未曾accept过任何提议，则会将Propalimid,null,nul)反馈给提议者。</p>
<ul>
<li><strong>accept 阶段</strong></li>
</ul>
<p>当提议者(Proposer)发 出prepare(N)后，若收到了超过半数的表决者(Accepter)的反馈，那么该提议者就会将其真正的提案Proposal(N,value)发送给所有的表决者。</p>
<p>当表决者(Acceptor)接收到提议者发送的Proposal(N,value)提案后，会再次拿出自己曾经accept过的提议中的最大编号maxN,及曾经反馈过的prepare的最大编号，让N与它们进行比较，若N大于等于这两个编号，则当前表决者accept 该提案，并反馈给提议者。若N小于这两个编号，则表决者采取不回应或回应Error的方式来拒绝该提议</p>
<p>若提议者 没有接收到超过半数的表决者的accept反馈，则重新进入prepare阶段，递增提案号，重新提出prepare请求。若提议者接收到的反馈数量超过了半数，则其它的未向提议者发送accept反馈的表决者将成为Learner,主动同步提议者的该提案。</p>
<h5 id="算法过程举例"><a class="markdownIt-Anchor" href="#算法过程举例"></a> 算法过程举例</h5>
<p>假设有三台主机，它们要从中选出一个Leader。这三台主机在不同的时间分别充当关提案的提议者Proposer、表决者Acceptor及学习者Learnor 三种不同的角色。</p>
<p>这里首先介绍一下该举例的前提:每个提议者(Proposer)都想提议自己要当Leader,假设三个提议者Proposer-1、Proposer-2、Proposer-3 提议的提案初始编号依次为20、10、30。每个提议者都要将提案发送给所有的表决者(Acceptor),为了便于理解，假设都只有两个(超过半数)表决者收到消息: Accepter-2 与Acceptor-3 收到了Proposer-2 的消息: Accepter-1与Acceptor-2收到了Proposer-1 的消息; Accepter-2与Acceptor-3收到了Proposer-3 的消息。</p>
<h4 id="paxos算法优化"><a class="markdownIt-Anchor" href="#paxos算法优化"></a> Paxos算法优化</h4>
<p>前面所述的Paxos算法在实际工程应用过程中，根据不同的实际需求存在诸多不便之处，所以也就出现了很多对于基本Paxos算法的优化算法，例如，Multi Paxos、Fast Paxos、EPaxos.而<strong>Zookeeper的Leader选举算法</strong>FastLeaderElection则<strong>是Fast Paxos 算法的工程应用</strong></p>
<h3 id="zab协议"><a class="markdownIt-Anchor" href="#zab协议"></a> ZAB协议</h3>
<p>ZAB，Zookeeper Atomic Broadcast, zk原子消息广播协议，是<strong>专为ZooKeeper设计的一种支持崩溃恢复的原子广播协议</strong>，是<strong>一种Pasox协议的优化算法</strong>。在Zookeeper中， 主要依赖ZAB协议来实现分布式数据一致性。</p>
<p>Zookeeper使用一个单一主进程来接收并处理客户端的所有事务请求，即写请求。当服务器数据的状态发生变更后，集群采用ZAB原子广播协议，以事务提案Proposal的形式广播到所有的副本进程上。ZAB协议能够保证一个全局的变更序列,即可以为每一个事务 分配一一个全局的递增编号xid.。</p>
<p>当Zookeeper客户端连接到Zookeeper集群的一个节点后，若客户端提交的是读请求，那么当前节点就直接根据自己保存的数据对其进行响应;如果是写请求且当前节点不是Leader,那么节点就会将该写请求转发给Leader, Leader会以提案的方式广播该写操作，只要有超过半数节点同意该写操作，则该写操作请求就会被提交。 然后Leader 会再次广播给所有订阅者，即Learner,通知它们同步数据</p>
<h4 id="三类角色"><a class="markdownIt-Anchor" href="#三类角色"></a> 三类角色</h4>
<p>为了避免Zookeeper的单点问题，zk也是以集群的形式出现的。集群中的角色主要有以下三类:</p>
<ul>
<li>
<p>Leader: zk集群写请求的唯一处理者， 并负责进行投票的发起和决议，更新系统状态。Leader是很民主的，并不是说其在接收到写请求后马上就修改其中保存的数据，而是首先根据写请求提出一个提议，在大多数zkServer均同意时才会做出修改。</p>
</li>
<li>
<p>Follower: 接收客户端请求，处理读请求，并向客户端返回结果，将写请求转给Leader，在选主(选Leader)过程中参与投票。</p>
</li>
<li>
<p>Obsenver:可以理解为无选主投票权与写操作投票权的Flollower,其不属于法定人数范围，主要是为了协助Follower处理更多的读请求。如果Zookeeper集群的读请求负载质高时，劳必要增加处理读请求的服务器数量:若增加的这些服务器都是以Follower的身份出现。则会大大降低写操作的效率。因为Leader发出的所有写操作提议，均需要通过法定人数半数以上同意。过多的Follwer会增加Leader与Follower的通信压力，降低写操作效率。同样，过多的Follower会延长Leader的选举时长，降低整个集群的可用性。此时，可选择增加Observer服务器，既提高了处理读操作的吞吐量。又没有墙加法定人数。只要法定人数不变，无论是写操作投票还是选主投票，其都不会增加通信压力，都不会影响投票效率。</p>
</li>
</ul>
<h4 id="三种模式"><a class="markdownIt-Anchor" href="#三种模式"></a> 三种模式</h4>
<p>ZAB协议中对zkServer的状态描述有三种模式:恢复模式、同步模式和广播模式。</p>
<ul>
<li>
<p><strong>恢复模式</strong>：在服务重启过程中，或在Leader崩溃后，就进入了恢复模式，要恢复到zk集群正常的工作状态</p>
</li>
<li>
<p><strong>同步模式</strong>：在所有的zkServer启动完毕，或Leader崩溃后又被选举出来时，就进入了同步模式，各个Follower需要马上将Leader 中的数据同步到自己的主机中。当大多数zkServer完成了与Leader的状态同步以后，恢复模式就结束了。所以，同步模式包含在恢复模式过程中。</p>
</li>
<li>
<p><strong>广播模式</strong>：当Leader的提议被大多数zkServer同意后，Leader会修改自身数据，然后将修改后的数据广播给其他Follower</p>
</li>
</ul>
<h4 id="zxid"><a class="markdownIt-Anchor" href="#zxid"></a> zxid</h4>
<p>zxid为64位长度的Long类型，其中高32位表示<strong>纪元epoch</strong>,低32位表示<strong>事务标识xid</strong>.即zxid由两部分构成: epoch 与xid.。</p>
<p>每个Leader都会具有一个不同的epoch值，表示一个时期、时代。每一次新的选举开启时都会生成一个新的epoch,新的Leader产生,则会更新所有zkServer 的zxid中的epoch。</p>
<p>xid则为zk的事务id,每一个写操作都是一个事务，都会有一个xid. xid为一个依次递增的流水号。每一个写操作都需要由Leader发起一一个提案，由所有Follower表决是否同意本次写操作，而每个提案都具有一个zxid。</p>
<h4 id="消息广播算法"><a class="markdownIt-Anchor" href="#消息广播算法"></a> 消息广播算法</h4>
<p>当集群中已经有过半的Follower与Leader服务器完成了状态同步，那么整个zk集群就可以进入消息广播模式了。</p>
<p>如果集群中的其他节点收到客户端的事务请求，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。Leader服务器会为其生成对应的事务提案Proposal, 并将其发送给集群中其余所有的主机，然后再分别收集它们的选票，在选票过半后进行事务提交。其具体过程如下:</p>
<ul>
<li>
<p>Leader接收到消息请求后，将消息赋予一个全局唯一的64位自增id, 即zxid,通过zxid的大小比较即可实现事务的有序性管理。</p>
</li>
<li>
<p>为了保证Leader向Follower发送提案的有序，Leader会为每个Follower创建一个 FIFO队列，并将提案副本写入到各个队列。然后再通过这些队列将提案发送给各个Follower.</p>
</li>
<li>
<p>当Follower接收到提案后，会先将提案的zxid与本地记录的事务日志中的最大的zxid进行比较。若当前提案的zxid大于最大zxid,则将当前提案记录到本地事务日志中，并向Leader返回一个ACK</p>
</li>
<li>
<p>当Leader接收到过半的ACKs后，对于之前回复过Leader的Follower, Leader 会向其发送COMMIT消息，批准这些Follower在本地执行该消息:对于之前未回复过Leader的Follower, Leader 会将这些Follower对应的队列中的提案发送给这些Follower,发送的同时会携带COMMIT消息。当Follower收到COMMIT消息后，就会执行该消息。</p>
</li>
</ul>
<h4 id="恢复模式两个原则"><a class="markdownIt-Anchor" href="#恢复模式两个原则"></a> 恢复模式两个原则</h4>
<p>当集群正在启动过程中，或Leader与超过半数的主机断连后，集群就进入了恢复模式。对于要恢复的数据状态需要遵循两个原则。</p>
<p><strong>已被处理过的消息不能丢</strong></p>
<p>当Leader收到超过半数Follower的ACKS后，就向各个Follower广播COMMIT 消息，批准各个Server执行该写操作事务。当各个Server在接收到Leader的COMMIT消息后就会在本地执行该写操作，然后会向客户端响应写操作成功。但是如果在非全部Follower 收到月  COMMIT消息之前Leader 就挂了，这将导致一种后果: 部分Server 已经执行了该事务，而部分Server尚未收到COMMIT消息，所以其并没有执行该事务。当新的Leader被选举出，集群经过恢复模式后需要保证所有Server上都执行了那些已经被部分Server执行过的事务。</p>
<p>为了保证“已被处理过的消息不能丢”的目的，ZAB的恢复模式使用了以下的策略:</p>
<ul>
<li>选举拥有proposal 最大值(即zxid 最大)的节点作为新的Leader: 由于所有提案被COMMIT之前必须有合法数量的Follower ACK,即必须有合法数量的服务器的事务日志上有该提案的proposal, 因此，只要有合法数量的节点正常工作，就必然有一个节点保  国中存了所有被COmMIT消息的proposal 状态。</li>
<li>新的Leader 先将自身拥有而并非所有Follower 都有的proposal 发送给Follower, 再将这些proposal的COMMIT命令发送给Follower,以保证所有的Follower 都保存并执行了所有的proposal.通过以上策略，能保证已经被处理的消息不会丢。</li>
</ul>
<p><strong>被丢弃的消息不能再现</strong></p>
<p>当Leader 接收到事务请求并生成了proposal,但还未向Follower发送时就挂了。由于其他Follower并没有收到此proposal,即并不知道该proposal的存在，因此在经过恢复模式重新选举产生了新的Leader后，这个事务被跳过。在整个集群尚未进入正常服务状态时，之前挂了的Leader主机重新启动并注册成为了Follower.但由于保留了被跳过的proposal,所以其与整个系统的状态是不一致的，需要将该proposal删除。</p>
<p>ZAB通过设计巧妙的zxid实现了这一目的。一个zxid是64位，高32是纪元epoch编号，每一次选举epoch的值都会增一。低32位是事务标识xid,每产生一个事务， 该xid值都会增一。这样设计的好处是旧的Leader挂了后重启，它不会被选举为新的Leader,因为此时它的zxid 肯定小于当前新的epoch。当旧的Leader作为Follower 接入新的Leader后，新的Leader会让其将所有旧的epoch 号的未被COMMIT的proposal 清除。</p>
<h4 id="leader选举算法paxos实现"><a class="markdownIt-Anchor" href="#leader选举算法paxos实现"></a> leader选举算法（paxos实现）</h4>
<p>当集群正在启动过程中，或Leader与超过半数的主机断连后，集群就进入了恢复模式。而恢复模式中最重要的阶段就是Leader 选举。</p>
<p>在集群 启动过程中的Leader选举过程(算法)与Leader断连后的Leader选举过程稍微有一些区别，基本相同</p>
<p><strong>集群启动中的 Leader 选举</strong></p>
<p>若进行Leader 选举，则至少需要两台主机，这里以三台主机组成的集群为例。。</p>
<p>在集群初始化阶段，当第台 服务器Server1启动时，其会给自己投票，然后发布自己的投票结果。投票包含所推举的服务器的myid和ZXID,使用(myid, ZXID)来表示，此时Server1的投票为(1，0)。由于其它机器还没有启动所以它收不到反馈信息，Server1 的状态一直属于Looking,即属于韭服务状态。。</p>
<p>当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader,选举过程如下:</p>
<ol>
<li>
<p>每个Server发出一个投票。此时Server1的投票为(1, 0), Server2的投票为(2,0),然后各自将这个投票发给集群中其他机器。，</p>
</li>
<li>
<p>接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。。</p>
</li>
<li>
<p>处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK, PK规则如下:。</p>
<ul>
<li>优先检查ZXID。ZXID 比较大的服务器优先作为Leader。。</li>
<li>如果ZXID相同，那么就比较myid。myid 较大的服务器作为Leader服务器。。对于Server1而言，它的投票是(1, 0),接收Server2的投票为(2, 0)。其首先会比较两者的ZXID,均为0，再比较myid,此时Server2的myid最大，于是Server1更新自己的投票为(2, 0),然后重新投票。对于Server2而言，其和须更新自己的投票，只是再次向集群中所有主机发出上一次投票信息即可。</li>
</ul>
</li>
<li>
<p>统计投票。每次投票后，每一台zkServer 都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息。对于Server1、Server2而言，都统计出集群中已经有两台主机接受了(2, 0)的投票信息，此时便认为已经选出了新的Leader,即Server2。</p>
</li>
<li>
<p>改变服务器状态。一旦确定了Leader, 每个服务器就会更新自己的状态，如果是Follower,那么就变更为FOLLOWING,如果是Leader,就变更为LEADING。</p>
</li>
<li>
<p>添加主机。在新的Leader选举出来后Server3启动，其想发出新一轮的选举。但由于当前集群中各个主机的状态并不是LOOKING，而是各司其职的正常服务，所以其只能是以Follower的身份加入到集群中。</p>
</li>
</ol>
<p><strong>断连后的Leader选举</strong></p>
<p>在Zookeeper运行期间，Leader 与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入时也不会影响Leader。但是若Leader服务器挂了，那么整个集群将暂停对外服务，进入新轮的Leader选举，其过程和启动时期的Leader选举过程基本一致。</p>
<p>假设正在运行的有Server1、Server2、 Server3 三台服务器，当前Leader是Server2,若某一时刻Server2挂了，此时便开始新一轮的Leader选举了。选举过程如下:</p>
<ol>
<li>变更状态。Leader挂后，余下的非Observer服务器都会将自己的服务器状态由FOLLOWING变更为LOOKING,然后开始进入Leader 选举过程。</li>
<li>每个Server 会发出一个投票，仍然会首先投自己。不过，在运行期间每个服务器上的ZXID可能是不同，此时假定Server1的ZXID为111. Server3 的ZXID为333;在第一轮投票中，Server1和Server3都会投自己，产生投票(1,111)，(3,333)， 然后各自将投票发送给集群中所有机器。</li>
<li>接收来自各个服务器的投票。与启动时过程相同。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。</li>
<li>处理投票。与启动时过程相同，此时，Server1将会成为Leader。</li>
<li>统计投票。与启动时过程相同。针对每一个投票， 服务器都需要将别人的投票和自己的投票进行PK。对于Server1而言，它的投票是(1, 111)，接收Server3的投票为(3, 333)。其首先会比较两者的ZXID, Server3投票的zxid为333大于Server1 投票的zxid的111,于是Server1更新自己的投票为(3, 333),然后重新投票。对于Server3而言，其无须更新自己的投票，只是再次向集群中所有主机发出上一.次投票信息即可。经过票数统计，最终Server3当选新的Leader。</li>
<li>改变服务器的状态。与启动时过程相同。一旦确定 了Leader,每个服务器就会更新国英」自己的状态。Server1 变更为FOLLOWING, Server3 变更为LEADING.</li>
</ol>
<h4 id="恢复模式下的数据同步"><a class="markdownIt-Anchor" href="#恢复模式下的数据同步"></a> 恢复模式下的数据同步</h4>
<p>当完成Leader选举后，就要进入到恢复模式下的数据同步阶段。Leader 服务器会为每一个 Follower 服务器准备一个队列，并将那些没有被各个Follower服务器同步的事务以Proposal的形式逐条发给各个Follower服务器，并在每一个Proposal后都紧跟一个commit消息，表示该事务已经被提交，Follower可以直接接收并执行。当follower服务器将所有尚未同步的事务proposal都从leader服务器同步过来并成功执行后，会向准leader发送ACK信息。leader服务器在收到该ACK后就会将该follower加入到真正可用的follower列表。</p>
<h3 id="cap原则"><a class="markdownIt-Anchor" href="#cap原则"></a> CAP原则</h3>
<p>CAP原则又称CAP定理，指的是在一个分布式系统中, Consistency(<strong>一致性</strong>) Availability(<strong>可用性</strong>)、Partition tolerance (<strong>分区容错性</strong>),三者不可兼得。<br />
      ●<strong>一致性</strong>©:分布式系统的所有主机中在同时刻是否可以保证具有完全相同的数据备份。若具有，则该分布式系统具有一致性。C<br />
      ●  <strong>可用性</strong>(A):在集群中部分节点发生故障后，是否会影响对客户端读写请求的响应。注意，若在短时间内会影响，其也不具有这里所说的“可用性”。山<br />
      ●  <strong>分区容错性</strong>(P):分布式系统中的一台主机称为一个分区。那么，什么是分布式系统的“错误”呢?分布式系统中各个主机无法保证数据的一致性是一 种错误; 分布式系统无法及时响应客户端请求，即不具有可用性也是一种错误。对于分布式系统，必须要具有对这些错误的“包容性”，即容错性，这是必须的。</p>
<h4 id="三二原则"><a class="markdownIt-Anchor" href="#三二原则"></a> 三二原则</h4>
<p>对于分布式系统，在CAP原则中分区容错性P是必须要保证的。但其并不能同时保证一致性与可用性。因为现在的分布式系统在满足了一致性的前提下，会牺牲可用性在满足了可用性的前提下，会牺牲一致性。所以，CAP原则对于一个分布式系统来说，只可能满足两项，即要么CP,要么AP。这就是CAP的三二原则。</p>
<p></p>
<h4 id="zk于cp"><a class="markdownIt-Anchor" href="#zk于cp"></a> zk于cp</h4>
<p>zk遵循的是CP原则，即保证了一致性，但牺牲了可用性。体现在哪里呢?。</p>
<p>当Leader宕机后，zk集群会马上进行新的Leader的选举。但选举时长在30-120秒间，整个选举期间zk集群是不接受客户端的读写操作的，即zk集群是处于瘫痪状态的。所以，其不满足可用性。</p>
<p>为什么Leader的选举需要这么长的时间呢?为了保证zk集群各个节点中数据的一致性，zk集群做了两类数据同步:初始化同步与更新同步。当新的Leader被选举出后，各个Follower需要将新Leader的数据同步到自己的缓存中，这是初始化同步;当Leader的数据被客户端修改后，其会向Follower发出广播，然后各个Follower会主动同步Leader的更新数据，这是更新同步。无论是初始化同步还是更新同步，zk 集群为了保证数据的一致性，若发现超过半数的Follower同步超时，则其会再次进行同步，而这个过程中zk集群是处于不可用状态的。</p>
<p>由于zk采用了CP原则，所以导致其可用性降低，这是其致命的问题。Spring Cloud的Eureka在分布式系统中所起的作用类似于zk，但其采用了AP原则，其牺牲了一致性，但保证了可用性</p>
<h2 id="zookeeper安装与集群搭建"><a class="markdownIt-Anchor" href="#zookeeper安装与集群搭建"></a> zookeeper安装与集群搭建</h2>
<h3 id="zookeeper环境准备"><a class="markdownIt-Anchor" href="#zookeeper环境准备"></a> zookeeper环境准备</h3>
<p><strong>下载</strong></p>
<p>下载完成后上传到Linux服务器</p>
<p><a target="_blank" rel="noopener" href="http://mirrors.hust.edu.cn/apache/zookeeper/">http://mirrors.hust.edu.cn/apache/zookeeper/</a></p>
<p><strong>解压</strong>：解压到/usr/local目录</p>
<pre><code class="highlight plaintext">tar -zxvf /home/leeboer/leeftp/zookeeper-3.4.13.tar.gz -C /usr/local/</code></pre>
<p>可执行的命令在zookeeper根目录的bin目录下：</p>
<p><strong>注册path</strong></p>
<p>注册之前我们先建立一个软连接（防止以后更换zookeeper要重新注册path）</p>
<pre><code class="highlight plaintext">ln -s ./zookeeper-3.4.13 /usr/local/zookeeper</code></pre>
<p>将zookeeper的bin目录注册到path目录下：</p>
<pre><code class="highlight plaintext">vim /etc/profile</code></pre>
<p>在文档末尾写如下配置：</p>
<pre><code class="highlight plaintext">export ZK_HOME=/usr/local/zookeeper
export PATH=$ZK_HOME/bin:$PATH</code></pre>
<p>然后重新加载该文件</p>
<pre><code class="highlight plaintext">source /etc/profile</code></pre>
<h3 id="配置zookeeper"><a class="markdownIt-Anchor" href="#配置zookeeper"></a> 配置zookeeper</h3>
<p>该文件为zookeeper的样例<code>/zookeeper/conf/zoo_sample.cfg</code></p>
<p>拷贝一份：</p>
<pre><code class="highlight plaintext">cp zoo_sample.cfg zoo.cfg</code></pre>
<p>然后编辑<code>zoo.cfg</code></p>
<pre><code class="highlight plaintext"># The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
# 快照存储的目录，(默认目录为/tmp/zookeeper)一定要更换！
dataDir=/usr/data/zookeeper
# the port at which the clients will connect
# 端口
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
#maxClientCnxns=60
#
# Be sure to read the maintenance section of the 
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to &quot;0&quot; to disable auto purge feature
#autopurge.purgeInterval=1 
~</code></pre>
<p><strong>启动zookeeper</strong></p>
<pre><code class="highlight plaintext">zkServer.sh start</code></pre>
<p><strong>重新启动</strong></p>
<pre><code class="highlight plaintext">zkServer.sh restart</code></pre>
<p><strong>关闭zookeeper</strong></p>
<pre><code class="highlight plaintext">zkServer.sh stop</code></pre>
<p><strong>查看zookeeper的状态</strong></p>
<pre><code class="highlight plaintext">zkServer.sh status</code></pre>
<p><strong>查看进程</strong></p>
<pre><code class="highlight plaintext">ps aux|grep aookeeper</code></pre>
<h2 id="zookeeper集群搭建"><a class="markdownIt-Anchor" href="#zookeeper集群搭建"></a> zookeeper集群搭建</h2>
<p>在刚才上面指定的zookeeper快照目录下<code>/usr/data/zookeeper</code></p>
<p>创建文件<code>myid</code>,内容写 1 就可以</p>
<pre><code class="highlight plaintext">echo 1 &gt; myid</code></pre>
<p><strong>配置zookeeper配置文件</strong></p>
<p><code>vim /usr/local/zookeeper/conf/zoo.cfg </code>加入：</p>
<ul>
<li>ip：连接端口号（数据同步时使用/随便写）：选举端口号（选举时使用/随便写）</li>
<li>如果需要指定observer，只需在最后加上 <code>:observer</code> 即可</li>
</ul>
<pre><code class="highlight plaintext">server.1=192.168.79.13:288:3888
server.2=192.168.79.14:288:3888
server.3=192.168.79.15:288:3888
server.4=192.168.79.16:288:3888:observer</code></pre>
<blockquote>
<p>server.1/server.2/server.3每个zookeeper的配置文件都是上述这样即可</p>
</blockquote>
<blockquote>
<p>注意：server.4是observer，需要在配置文件中单独加一段配置</p>
</blockquote>
<pre><code class="highlight plaintext">peerType=observer</code></pre>
<p><strong>然后依次启动各个zookeeper即可</strong></p>
<pre><code class="highlight plaintext">zkServer.sh start</code></pre>
<blockquote>
<p>第一台启动时查看状态</p>
</blockquote>
<blockquote>
<p>第二台启动时查看状态</p>
</blockquote>
<blockquote>
<p>第三台启动时查看状态</p>
</blockquote>
<blockquote>
<p>当第二台挂掉后第三台自动选举为leader（因为myid大）</p>
</blockquote>
<h2 id="leader的选举机制"><a class="markdownIt-Anchor" href="#leader的选举机制"></a> leader的选举机制</h2>
<h3 id="myid"><a class="markdownIt-Anchor" href="#myid"></a> myid</h3>
<p>这是zk集群中服务器的唯一标识，称为myid。例如：有三个zk服务器，那么编号分别为1，2，3</p>
<h3 id="逻辑时钟"><a class="markdownIt-Anchor" href="#逻辑时钟"></a> 逻辑时钟</h3>
<p>逻辑时钟，Logicalock，是一个整型树，改概念在选举时称为logicalclock，而在zxid中则为epoch的值，即epoch和logicalclock是同一个值，在不同情况下的不同名称</p>
<h3 id="zk状态"><a class="markdownIt-Anchor" href="#zk状态"></a> zk状态</h3>
<ul>
<li><strong>LOOKING</strong>：选举状态（查找leader的状态）</li>
<li><strong>FOLLOWING</strong>：跟随状态，同步leader状态，处于该状态的服务器称为Follower。</li>
<li><strong>OBSERVING</strong>：观察状态，同步leader状态，处于该状态的服务器称为Observer</li>
<li><strong>LEADING</strong>：领导状态。处于该状态的服务器称为Leader</li>
</ul>
</main>

</article>


<script src="/js/highlight.js"></script>

  </main>
  <footer class="footer">
  
  <span>Copyright © 2026 Lism Blog | lisisism@qq.com</span>
  
</footer>
  
<script src="/js/theme.js"></script>

</body>

</html>